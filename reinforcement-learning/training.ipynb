{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import pygame \n",
    "import matplotlib.pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Definiton\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "print(observation)\n",
    "print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function definion\n",
    "\n",
    "# Model Definition\n",
    "\n",
    "class LunarLanderRL(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LunarLanderRL, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation = 'relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(64, activation = 'relu')\n",
    "        self.classifier = tf.keras.layers.Dense(4, activation = 'softmax')\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining one Episode\n",
    "\n",
    "def Episode(model = tf.keras.Model):\n",
    "    observation, info = env.reset()\n",
    "    reward = 0\n",
    "    while(True):\n",
    "        model.predict()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode = 'human')\n",
    "model = LunarLanderRL()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def train_step(initial_state, action, reward, next_state, done):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict action probabilities\n",
    "        action_probs = model(initial_state)\n",
    "        # Compute the log probability of the chosen action\n",
    "        log_prob = tf.math.log(action_probs[0, action])\n",
    "        # Compute the advantage\n",
    "        advantage = reward - tf.reduce_max(action_probs)\n",
    "        # Compute loss value as the negative log probability of the chosen action, weighted by the advantage\n",
    "        loss_value = -log_prob * advantage\n",
    "        # If episode is done, apply a penalty to the loss\n",
    "        if done:\n",
    "            loss_value += 100  # Penalty value can be tuned\n",
    "    # Compute gradient and apply it to the model\n",
    "    print(loss_value)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_value\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1000):  # Number of episodes can be tuned\n",
    "    initial_state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "       # Reshape the state to have an extra dimension\n",
    "        state = np.reshape(initial_state, [1, -1])\n",
    "        action_probs = model.predict(state, verbose=None)\n",
    "        action = np.random.choice(range(env.action_space.n), p=action_probs.ravel())\n",
    "        next_state, reward, done, _, _= env.step(action)\\\n",
    "        # Also reshape the next_state before passing it to train_step\n",
    "        next_state = np.reshape(next_state, [1, -1])\n",
    "        loss_value = train_step(state, action, reward, next_state, done)\n",
    "        initial_state = next_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
