{
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C3_W1_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "tensorflow-datasets-w1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
=======
>>>>>>> 5c5c918 (courses)
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
<<<<<<< HEAD
        "id": "view-in-github",
        "colab_type": "text"
=======
        "colab_type": "text",
        "id": "view-in-github"
>>>>>>> 5c5c918 (courses)
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-2-public/blob/adding_C3/C3/W1/assignment/C3_W1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1r94VUw80m9"
      },
      "source": [
        "### Step 0 - Setup\n",
        "\n",
        "**Note** : The assignment uses TF version 2 so if you run this notebook on TF 1.x, some things might not work."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "O9tHqv3u8nkZ"
      },
=======
      "execution_count": 1,
      "metadata": {
        "id": "O9tHqv3u8nkZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-22 17:26:02.651511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 17:26:03.775340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "from os import getcwd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
<<<<<<< HEAD
      ],
      "execution_count": null,
      "outputs": []
=======
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6vH4pRk8nka"
      },
      "source": [
        "### Step 1 - One-Hot Encoding \n",
        "\n",
        "Remember to one hot encode the labels as you have 3 classes - Rock, Paper and Scissors.\n",
        "You can use Tensorflow's one_hot function ([`tf.one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot)) to convert categorical variables to one-hot vectors.\n",
        "\n",
        "Useful parameters - \n",
        "1. `indices` - A tensor containing all indices\n",
        "2. `depth` - A scalar defining the depth of the one hot dimension.\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "r3QZ5vp28nka"
      },
=======
      "execution_count": 3,
      "metadata": {
        "id": "r3QZ5vp28nka"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 0.]\n",
            " [0. 1. 0.]], shape=(4, 3), dtype=float32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-22 17:27:51.855811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.119450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.119656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.121735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.121899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.122042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.213883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.214069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.214216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 17:27:52.214365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2251 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "# EXERCISE: encoding the labels using your own function for one-hot encoding\n",
        "\n",
        "def my_one_hot(feature, label):\n",
        "    # Encode the labels to one-hot using tf.one_hot with depth equal to total \n",
        "    # number of classes here which are rock, paper and scissors\n",
        "    \n",
<<<<<<< HEAD
        "    one_hot = # YOUR CODE HERE\n",
=======
        "    one_hot = tf.one_hot(label, depth = 3) # YOUR CODE HERE\n",
>>>>>>> 5c5c918 (courses)
        "    return feature, one_hot\n",
        "\n",
        "# TESTING THE FUNCTION \n",
        "_,one_hot = my_one_hot([\"a\",\"b\",\"c\",\"a\"],[1,2,3,1])\n",
        "print(one_hot)\n"
<<<<<<< HEAD
      ],
      "execution_count": null,
      "outputs": []
=======
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yINlAX_98nka"
      },
      "source": [
        "#### Expected Output\n",
        "```\n",
        "tf.Tensor(\n",
        "[[0. 1. 0.]\n",
        " [0. 0. 1.]\n",
        " [0. 0. 0.]\n",
        " [0. 1. 0.]], shape=(4, 3), dtype=float32)\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPFbTls8nka"
      },
      "source": [
        "### Step 2 - Loading Dataset\n",
        "\n",
        "You will be using [`tfds.load()`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)] method to load the dataset. The dataset is already downloaded and unzipped for you in the data folder but if you are running on your local machine and do not have the dataset downloaded, it will first download and save the dataset to your tensorflow directory and then load it.\n",
        "\n",
        "Useful parameters -\n",
        "1. `split` - Which split of the data to load (e.g. 'train', 'test' ['train', 'test'], 'train[80%:]',...)\n",
        "\n",
        "2. `data_dir` - Directory to read/write data. Defaults to the value of the environment variable _TFDS_DATA_DIR_, if set, otherwise falls back to \"~/tensorflow_datasets\"\n",
        "\n",
        "3. `as_supervised`- If True, the returned tf.data.Dataset will have a 2-tuple structure (input, label) according to builder.info.supervised_keys. If False the default, the returned tf.data.Dataset will have a dictionary with all the features\n",
        "\n",
        "**Note** - The`rock_paper_scissors:3.*.*` dataset is already downloaded for you so if you specify the major version this way while loading, it will try to find the dataset from the directory and load it. If none is present or the current dataset has been upgraded to a new major version, then it will try to download the new dataset to the directory."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "7EZtQBxK8nkb"
      },
=======
      "execution_count": 4,
      "metadata": {
        "id": "7EZtQBxK8nkb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-22 17:30:22.883147: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset 219.53 MiB (download: 219.53 MiB, generated: Unknown size, total: 219.53 MiB) to /home/blank/tensorflow_datasets/rock_paper_scissors/3.0.0...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83bcc28f3dee469fb438fc1c069f2c4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "995cd5ca35284492a9cfd93ea91aa1d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef725d81a6de4f77b9edba8db4a708dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb428638234942b2ada286c4334d9a96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...:   0%|          | 0/2520 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e1dcc8ef72485399bdfe269fd4defe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /home/blank/tensorflow_datasets/rock_paper_scissors/3.0.0.incompleteF7P2Z2/rock_paper_scissors-train…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7abc25607fbb44708b9e600348095abe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test examples...:   0%|          | 0/372 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d4c68a6042e489b8aaba0b3ae2ffa4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /home/blank/tensorflow_datasets/rock_paper_scissors/3.0.0.incompleteF7P2Z2/rock_paper_scissors-test.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset rock_paper_scissors downloaded and prepared to /home/blank/tensorflow_datasets/rock_paper_scissors/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "2520\n",
            "372\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "# EXERCISE: Loading the rock, paper and scissors train and test dataset using tfds.load. \n",
        "\n",
        "# Use data_dir=filepath as the dataset is already downloaded for you\n",
        "\n",
        "# use 'filePath' if you are running on Coursera, ignore 'filePath' if you are running on Colab or your local machine\n",
        "filePath = f\"{getcwd()}/data\"\n",
        "\n",
<<<<<<< HEAD
        "train_data = tfds.load(# YOUR CODE HERE)\n",
        "val_data = tfds.load(# YOUR CODE HERE)\n",
=======
        "train_data = tfds.load('rock_paper_scissors', split = 'train')\n",
        "val_data = tfds.load('rock_paper_scissors', split = 'test')\n",
>>>>>>> 5c5c918 (courses)
        "\n",
        "# Testing train_data and val_data if loaded correctly\n",
        "    \n",
        "train_data_len = len(list(train_data))\n",
        "val_data_len = len(list(val_data))\n",
        "\n",
        "print(train_data_len)\n",
        "print(val_data_len)"
<<<<<<< HEAD
      ],
      "execution_count": null,
      "outputs": []
=======
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POfTR6Ow8nkb"
      },
      "source": [
        "#### Expected Output\n",
        "```\n",
        "2520\n",
        "372\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq4p4M9C8nkc"
      },
      "source": [
        "### Step 3 - Mapping one hot encode function to dataset\n",
        "\n",
        "You will apply the `my_one_hot()` encoding function to the train and validation data using [`map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) function. It will apply the custom function to each element of the  dataset and returns a new dataset containing the transformed elements in the same order as they appeared in the input."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "4skEJsdt8nkc"
      },
=======
      "execution_count": 8,
      "metadata": {
        "id": "4skEJsdt8nkc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.map_op._MapDataset'>\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "# EXERCISE: one-hot encode the train and validation labels using the function you defined earlier\n",
        "\n",
        "# HINT - use map function https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
        "\n",
<<<<<<< HEAD
        "train_data = # YOUR CODE HERE\n",
        "val_data = # YOUR CODE HERE\n",
        "\n",
        "print(type(train_data))"
      ],
      "execution_count": null,
      "outputs": []
=======
        "train_data = train_data.map(lambda x: my_one_hot(x['image'], x['label'])) \n",
        "val_data =  val_data.map(lambda x: my_one_hot(x['image'], x['label']))\n",
        "\n",
        "print(type(train_data))"
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIcMztvz8nkc"
      },
      "source": [
        "#### Expected Output\n",
        "```\n",
        "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrsTKJEz8nkc"
      },
      "source": [
        "### Step 4 - Exploring dataset metadata\n",
        "\n",
        "Do remember that [`tfds.load()`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) has a parameter called `with_info` which if True will return the tuple (tf.data.Dataset, tfds.core.DatasetInfo) containing the info associated with the builder."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "wb8o45GA8nkd"
      },
=======
      "execution_count": 9,
      "metadata": {
        "id": "wb8o45GA8nkd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300, 300, 3)\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "# EXERCISE: Check the information about the dataset to see the dataset image shape\n",
        "\n",
        "# HINT: Use with_info=True and data_dir (use 'data_dir' if running on Coursera, otherwise skip that parameter)\n",
<<<<<<< HEAD
        "_,info = tfds.load(# YOUR CODE HERE)\n",
        "\n",
        "# DO NOT EDIT THIS\n",
        "print(info.features['image'].shape)"
      ],
      "execution_count": null,
      "outputs": []
=======
        "_,info = tfds.load('rock_paper_scissors', with_info= True)\n",
        "\n",
        "# DO NOT EDIT THIS\n",
        "print(info.features['image'].shape)"
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l2aQePv8nkd"
      },
      "source": [
        "## Expected Output\n",
        "```\n",
        "(300, 300, 3)\n",
        "\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6wmrUGs8nkd"
      },
      "source": [
        "### Step 5 - Training your simple CNN classifier\n",
        "\n",
        "Now you will define a simple 1-layer CNN model which will learn to classify the images into rock, paper and scissor!\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "iSq4t32ZHHpt"
      },
=======
      "execution_count": 11,
      "metadata": {
        "id": "iSq4t32ZHHpt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 149, 149, 16)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 355216)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 1065651   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1066099 (4.07 MB)\n",
            "Trainable params: 1066099 (4.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
>>>>>>> 5c5c918 (courses)
      "source": [
        "# EXERCISE: Train a simple CNN model on the dataset\n",
        "\n",
        "train_batches = train_data.shuffle(100).batch(10)\n",
        "validation_batches = val_data.batch(32)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
<<<<<<< HEAD
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=# YOUR CODE HERE),                       \n",
        "    # YOUR CODE HERE - Add a maxpool2d layer with kernel (2,2) \n",
        "    # YOUR CODE HERE - Add a flatten layer                                           \n",
        "    tf.keras.layers.Dense(# YOUR CODE HERE)  # Remember there are 3 classes to classify and to use proper activation\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
=======
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "    # YOUR CODE HERE - Add a maxpool2d layer with kernel (2,2) \n",
        "    # YOUR CODE HERE - Add a flatten layer                                           \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # Remember there are 3 classes to classify and to use proper activation\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCH-5UT88nke"
      },
      "source": [
        "# Submission Instructions"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "XTi8sPqu8nkf"
      },
      "source": [
        "# Now click the 'Submit Assignment' button above."
      ],
      "execution_count": null,
      "outputs": []
=======
      "execution_count": null,
      "metadata": {
        "id": "XTi8sPqu8nkf"
      },
      "outputs": [],
      "source": [
        "# Now click the 'Submit Assignment' button above."
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkt1TG328nkf"
      },
      "source": [
        "### [Optional] Step 6 - Evaluation\n",
        "\n",
        "***Remember to submit your assignment before you uncomment and run the next cell.***"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "si9y93Xr8nkf"
      },
      "source": [
        "# # OPTIONAL EXERCISE: Compile and fit your model - use categorical loss and choose optimizer as Adam\n",
        "\n",
        "# EPOCH = 3\n",
        "\n",
        "# # You should get decent enough training accuracy in 3-4 epochs itself as this one layer model will heavily overfit\n",
        "\n",
        "# model.compile(# YOUR CODE HERE)\n",
        "\n",
        "# history = model.fit(train_batches, epochs= EPOCH , validation_data=validation_batches, validation_steps=1)\n",
        "    \n",
        "# print(\"Final Training Accuracy:-\",history.history['accuracy'][-1])\n",
        "# print(\"Final Validation Accuracy:-\",history.history['val_accuracy'][-1])"
      ],
      "execution_count": null,
      "outputs": []
=======
      "execution_count": 14,
      "metadata": {
        "id": "si9y93Xr8nkf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "252/252 [==============================] - 3s 9ms/step - loss: 5.5728 - accuracy: 0.9667 - val_loss: 48.6098 - val_accuracy: 0.4688\n",
            "Epoch 2/3\n",
            "252/252 [==============================] - 2s 9ms/step - loss: 0.2254 - accuracy: 0.9960 - val_loss: 42.0180 - val_accuracy: 0.5312\n",
            "Epoch 3/3\n",
            "252/252 [==============================] - 2s 9ms/step - loss: 0.0390 - accuracy: 0.9992 - val_loss: 28.9099 - val_accuracy: 0.5312\n",
            "Final Training Accuracy:- 0.9992063641548157\n",
            "Final Validation Accuracy:- 0.53125\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL EXERCISE: Compile and fit your model - use categorical loss and choose optimizer as Adam\n",
        "\n",
        "EPOCH = 3\n",
        "\n",
        "# You should get decent enough training accuracy in 3-4 epochs itself as this one layer model will heavily overfit\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
        "\n",
        "history = model.fit(train_batches, epochs= EPOCH , validation_data=validation_batches, validation_steps=1)\n",
        "    \n",
        "print(\"Final Training Accuracy:-\",history.history['accuracy'][-1])\n",
        "print(\"Final Validation Accuracy:-\",history.history['val_accuracy'][-1])"
      ]
>>>>>>> 5c5c918 (courses)
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pYYIdAO8nkf"
      },
      "source": [
        "# When you're done or would like to take a break, please run the two cells below to save your work and close the Notebook. This frees up resources for your fellow learners."
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "metadata": {
        "id": "RBpzAfBF8nkh"
      },
=======
      "execution_count": null,
      "metadata": {
        "id": "RBpzAfBF8nkh"
      },
      "outputs": [],
>>>>>>> 5c5c918 (courses)
      "source": [
        "%%javascript\n",
        "<!-- Save the notebook -->\n",
        "IPython.notebook.save_checkpoint();"
<<<<<<< HEAD
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxwyA9xe8nkh"
      },
=======
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxwyA9xe8nkh"
      },
      "outputs": [],
>>>>>>> 5c5c918 (courses)
      "source": [
        "%%javascript\n",
        "<!-- Shutdown and close the notebook -->\n",
        "window.onbeforeunload = null\n",
        "window.close();\n",
        "IPython.notebook.session.delete();"
<<<<<<< HEAD
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
=======
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "C3_W1_Assignment.ipynb",
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "tensorflow-datasets-w1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 5c5c918 (courses)
